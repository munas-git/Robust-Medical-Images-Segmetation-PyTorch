{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3043853,"sourceType":"datasetVersion","datasetId":1864103}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Info about notebook...\n\nThis notebook... (Fill out later)\n\n### Dataset Notes\n**Retinal Image Dataset**\n- Dataset can be found [here.](https://www.kaggle.com/datasets/ipythonx/retinal-vessel-segmentation) ***File size = 648.3 MB***\n- Contains two mask types: Eye mask, and Retina mask.\n- Only working with data in CHASE_DB1, and HRF\n- Every single image in `CHASE_DB1/Images` has two masks in `CHASE_DB1/Masks`\n- The masks within `CHASE_DB1/Masks` are represented as True/False **and not** 1/0\n\n**Lung Segmentation Dataset**\n- The dataset can be found or [here](https://github.com/IlliaOvcharenko/lung-segmentation/tree/master) or directly through this [link.](https://drive.google.com/file/d/1ffbbyoPf-I3Y0iGbBahXpWqYdGd7xxQQ/view?usp=sharing) ***File size = 4.19GB***\n\n**Skin Lesion Segmentation Dataset**\n- The dataset can be found or [here](https://challenge.isic-archive.com/data/#2017) or directly through this [link.](https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Data.zip) The masks here are in superpixel format ***File size = 5.8GB***. Binary masks can be found [here](https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Part1_GroundTruth.zip) ***File size = 9MB***","metadata":{}},{"cell_type":"markdown","source":"# Library downloads.","metadata":{}},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Important Libraries","metadata":{}},{"cell_type":"code","source":"# File System Handling\nimport os\nimport gdown\nimport tarfile\nimport zipfile\n\n# Data Processing\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\n\n# Image Visualization\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Model Training, evaluation processes...\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\n\n# Handling Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Handling OutOfMemoryError\n# import multiprocessing\n# multiprocessing.set_start_method(\"spawn\", force = True)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n\n#########################################\n# import torch.multiprocessing as mp\n# mp.set_sharing_strategy('file_system')\n######################################### Re-run and change num workers back to >1.\n\nprint(\"All libraries succesfully imported, with configorations implemented.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Organizing Train images path","metadata":{}},{"cell_type":"markdown","source":"## Image Paths or Url","metadata":{}},{"cell_type":"code","source":"# Declaring all paths, and URLs.\n\n# **Retina Images**\nEYE_IMAGES_1 = \"/kaggle/input/retinal-vessel-segmentation/CHASE_DB1/Images\"\nEYE_IMAGES_2 = \"/kaggle/input/retinal-vessel-segmentation/HRF/images\"\n\nEYE_MASKS_1 = \"/kaggle/input/retinal-vessel-segmentation/CHASE_DB1/Masks\"\nEYE_MASKS_2 = \"/kaggle/input/retinal-vessel-segmentation/HRF/manual1\"\n\n# **Lungs Images**\n# Sharing URL modified to Download URL\n# FROM THIS FORMAT BELOW\n# https://drive.google.com/file/d/FILE_ID/view?usp=sharing\n# TO THIS FORMAT BELOW\n# https://drive.google.com/uc?export=download&id=FILE_ID\nLUNG_IMAGES = \"https://drive.google.com/uc?export=download&id=1ffbbyoPf-I3Y0iGbBahXpWqYdGd7xxQQ\"\nLUNG_BASE_DIR = \"/kaggle/working/lungs/\"\nLUNG_IMAGES_DIR = os.path.join(LUNG_BASE_DIR, \"dataset\", \"images\")\nLUNG_MASKS_DIR = os.path.join(LUNG_BASE_DIR, \"dataset\", \"masks\")\n\n# **Skin Lesion Images**\nSKIN_LESION_BASE_DIR = \"/kaggle/working/skin_lesion\"\nSKIN_LESION_IMAGES = \"https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Data.zip\"\nSKIN_LESION_BINARY_MASKS = \"https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Part1_GroundTruth.zip\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample of an image an eye image and its two labels.\n# Notice how they slightly differ?\n# I will include both samples in training data as some form of\n# augmentation as well.\n\n# 0 > i < 14\ni = \"14\" # Adjust to see different images. Numbers below 10 start with 0 e.g \"02\"\n\nplt.subplots(1,3, figsize = (10,10))\n\nplt.subplot(1,3,1)\nplt.title(\"Eye Image\")\nim = Image.open(os.path.join(EYE_IMAGES_1, f\"Image_{i}R.jpg\"))\nplt.imshow(im)\nplt.axis(False);\n\nplt.subplot(1,3,2)\nplt.title(\"Eye Mask 1\")\nim = Image.open(os.path.join(EYE_MASKS_1, f\"Image_{i}R_1stHO.png\"))\nplt.axis(False)\nplt.imshow(im, cmap='gray')\n\nplt.subplot(1,3,3)\nplt.title(\"Eye Mask 2\")\nim = Image.open(os.path.join(EYE_MASKS_1, f\"Image_{i}R_2ndHO.png\"))\nplt.axis(False)\nplt.imshow(im, cmap='gray');\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Retina Images","metadata":{}},{"cell_type":"code","source":"# ============== Sorting Images and masks in CHASE_DB1 ============== #\nimage_paths = [] # list of images.\nmask_paths = [] # list of corresponding image masks.\n\ncount = 0\n\nfor image in os.listdir(EYE_IMAGES_1): # Looping through every image\n    image_masks = [] # List to store the two mask paths per image\n    \n    for mask in os.listdir(EYE_MASKS_1): # Looping through all masks\n        if image[:9] in mask: # Checking if mask name has image name in it\n            image_paths.append(os.path.join(EYE_IMAGES_1, image)) # Adding image path\n            mask_paths.append(os.path.join(EYE_MASKS_1, mask)) # Adding corresponding mask path\n            \nimage_paths = sorted(image_paths)\nmask_paths = sorted(mask_paths)\n\n# ============== Sorting Images and masks in HRF ============== #\nretina_image_paths = sorted(image_paths + [os.path.join(EYE_IMAGES_2, path) for path in os.listdir(EYE_IMAGES_2)])\nretina_mask_paths = sorted(mask_paths + [os.path.join(EYE_MASKS_2, path) for path in os.listdir(EYE_MASKS_2)])\n\nprint(f\"Rentina:\\nThere are {len(retina_image_paths)} training images, and {len(retina_mask_paths)} masks.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lungs Images","metadata":{}},{"cell_type":"code","source":"# Creating base dir for lung images\nos.makedirs(LUNG_BASE_DIR, exist_ok = True)\n\n# Downloading dataset.\ngdown.download(LUNG_IMAGES, LUNG_BASE_DIR, quiet = False)\n\n# Extracting downloaded dataset\nwith tarfile.open(\"/kaggle/working/lungs/dataset.tar.gz\", \"r:gz\") as zip_ref:\n    zip_ref.extractall(LUNG_BASE_DIR)\n    \n# Deleting zipped file\nos.remove(os.path.join(LUNG_BASE_DIR, \"dataset.tar.gz\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sorting Train Image Paths\nlung_image_paths = sorted([os.path.join(LUNG_IMAGES_DIR, path) for path in os.listdir(LUNG_IMAGES_DIR)])\nlung_mask_paths = sorted([os.path.join(LUNG_MASKS_DIR, path) for path in os.listdir(LUNG_MASKS_DIR)])\n\nprint(f\"Lungs:\\nThere are {len(lung_image_paths)} training images, and {len(lung_image_paths)} masks.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample of a lung image and label.\n\n# 0 >= i <= 703\ni = 703 # Adjust to see different images.\n\nplt.subplots(1,2, figsize = (10,10))\n\nplt.subplot(1,2,1)\nplt.title(\"Lung Image\")\nim = Image.open(lung_image_paths[i])\nplt.imshow(im, cmap = \"gray\")\nplt.axis(False);\n\nplt.subplot(1,2,2)\nplt.title(\"Lung Mask\")\nim = Image.open(lung_mask_paths[i])\nplt.axis(False)\nplt.imshow(im, cmap = \"gray\")\n\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Skin lesion Images","metadata":{}},{"cell_type":"code","source":"# Creating base dir for lung images\nos.makedirs(SKIN_LESION_BASE_DIR, exist_ok = True)\n\n# Downloading dataset.\n!wget --no-check-certificate '{SKIN_LESION_IMAGES}' -O {os.path.join(SKIN_LESION_BASE_DIR, \"dataset.zip\")}\n\n# Extracting downloaded dataset\nwith zipfile.ZipFile(os.path.join(SKIN_LESION_BASE_DIR, \"dataset.zip\"), 'r') as zip_ref:\n    zip_ref.extractall(SKIN_LESION_BASE_DIR)\n    \n# Deleting zipped file\nos.remove(os.path.join(SKIN_LESION_BASE_DIR, \"dataset.zip\"))\n\n# Downloading binary masks.\n!wget --no-check-certificate '{SKIN_LESION_BINARY_MASKS}' -O {os.path.join(SKIN_LESION_BASE_DIR, \"bin_mask.zip\")}\n\n# Extracting binary masks\nwith zipfile.ZipFile(os.path.join(SKIN_LESION_BASE_DIR, \"bin_mask.zip\"), 'r') as zip_ref:\n    zip_ref.extractall(SKIN_LESION_BASE_DIR)\n    \n# Deleting zipped file\nos.remove(os.path.join(SKIN_LESION_BASE_DIR, \"bin_mask.zip\"))\n\n# Moving csv file out to avoid mismatch issues later.\nos.rename(os.path.join(SKIN_LESION_BASE_DIR, \"ISIC-2017_Training_Data\", \"ISIC-2017_Training_Data_metadata.csv\"), os.path.join(SKIN_LESION_BASE_DIR, \"ISIC-2017_Training_Data_metadata.csv\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_ = pd.read_csv(os.path.join(SKIN_LESION_BASE_DIR, \"ISIC-2017_Training_Data_metadata.csv\"))\ncsv_.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sorting Train Image Paths\nlesion_image_paths = []\nlesion_mask_paths = []\n\n# ADJUST BAD LOGIC HERE LATER ;)\n# RUN THIS ABOUT 3 TIMES TO HAVE THE FULL DATASET.\nfor path, bin_path in zip(\n    os.scandir(os.path.join(SKIN_LESION_BASE_DIR, \"ISIC-2017_Training_Data\")),\n    os.scandir(os.path.join(SKIN_LESION_BASE_DIR, \"ISIC-2017_Training_Part1_GroundTruth\"))\n):\n    \n    # Adding all binary label paths to list.\n    lesion_mask_paths.append(os.path.join(SKIN_LESION_BASE_DIR, bin_path))\n    \n    if path.name.endswith(\"jpg\"): # Checking if the picture is jpg meaning image (superpixel masks)\n        lesion_image_paths.append(os.path.join(SKIN_LESION_BASE_DIR, path)) # Adding all image paths to list\n    \n    elif path.name.endswith(\"png\"): # Deleting \"superpixel masks\"\n        os.remove(os.path.join(SKIN_LESION_BASE_DIR, path))\n        \n# # Sorting both lists so that images and labels match.\nlesion_image_paths, lesion_mask_paths = sorted(lesion_image_paths), sorted(lesion_mask_paths)\n\nprint(f\"Skin Lesion:\\nThere are {len(lesion_image_paths)} training images, and {len(lesion_mask_paths)} masks.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample of a lesion image and label.\n\n# 0 >= i <= 1999\ni = 8 # Adjust to see different images.\n\nplt.subplots(1,2, figsize = (10,10))\n\nplt.subplot(1,2,1)\nplt.title(\"Lesion Image\")\nimage = Image.open(lesion_image_paths[i])\nplt.imshow(image, cmap = \"gray\")\nplt.axis(False);\n\nplt.subplot(1,2,2)\nplt.title(\"Lesion Mask\")\nmask = Image.open(lesion_mask_paths[i])\nplt.axis(False)\nplt.imshow(mask, cmap = \"gray\")\n\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-processing & Augmentation","metadata":{}},{"cell_type":"code","source":"# Defining Standardz.\nIMAGE_SIZE = (512, 512)\nBATCH_SIZE = 2\nTEST_SIZE = 0.1 # Percentage of data to use for testing.\nVAL_SIZE = 0.1 # Percentage of data to use for evaluation (During Training).\nNUM_CLASSES = 2 # 2 because all masks are binary.\nR_FULL = len(retina_image_paths) # Retina full data amount \nLU_FULL = len(lung_image_paths) # Lungs full data amount\nLE_FULL = len(lesion_image_paths) # Lesion full data amount\n\n# Calculating TEST_SIZE, and VAL_SIZE for each data set.\nretina_v_per = int(len(retina_image_paths) * VAL_SIZE) # Validation size\nlung_v_per = int(len(lung_image_paths) * VAL_SIZE) # Validation size\nlesion_v_per = int(len(lesion_image_paths) * VAL_SIZE) # Validation size\n\nretina_t_per = int(len(retina_image_paths) * TEST_SIZE) # Test size\nlung_t_per = int(len(lung_image_paths) * TEST_SIZE) # Test size\nlesion_t_per = int(len(lesion_image_paths) * TEST_SIZE) # Test size\n\n# Extracting Training, Test, and Validation data... Extracting first, and last n file paths.\nretina_train_images, retina_train_masks = retina_image_paths[:R_FULL - (retina_v_per + retina_t_per)], retina_mask_paths[:R_FULL - (retina_v_per + retina_t_per)] # First \"n\" file paths\nretina_val_images, retina_val_masks = retina_image_paths[- (retina_v_per + retina_t_per): - retina_t_per], retina_mask_paths[- (retina_v_per + retina_t_per): - retina_t_per] # Next \"n\" starting at first n + 1 and stopping before last.\nretina_test_images, retina_test_masks = retina_image_paths[- retina_t_per:], retina_mask_paths[- retina_t_per:] # Last \"n\"\n\nlung_train_images, lung_train_masks = lung_image_paths[:LU_FULL - (lung_v_per + lung_t_per)], lung_mask_paths[:LU_FULL - (lung_v_per + lung_t_per)] # First \"n\" file paths\nlung_val_images, lung_val_masks = lung_image_paths[- (lung_v_per + lung_t_per): - lung_t_per], lung_mask_paths[- (lung_v_per + lung_t_per): - lung_t_per] # Next \"n\" starting at first n + 1 and stopping before last.\nlung_test_images, lung_test_masks = lung_image_paths[- lung_t_per:], lung_mask_paths[- lung_t_per:] # Last \"n\"\n\nlesion_train_images, lesion_train_masks = lesion_image_paths[:LE_FULL - (lesion_v_per + lesion_t_per)], lesion_mask_paths[:LE_FULL - (lesion_v_per + lesion_t_per)] # First \"n\" file paths\nlesion_val_images, lesion_val_masks = lesion_image_paths[- (lesion_v_per + lesion_t_per): - lesion_t_per], lesion_mask_paths[- (lesion_v_per + lesion_t_per): - lesion_t_per] # Next \"n\" starting at first n + 1 and stopping before last.\nlesion_test_images, lesion_test_masks = lesion_image_paths[- lesion_t_per:], lesion_mask_paths[- lesion_t_per:] # Last \"n\"\n\nfull_train_image, full_train_mask = sorted((retina_train_images + lung_train_images + lesion_train_images)), sorted((retina_train_masks + lung_train_masks + lesion_train_masks))\nfull_val_image, full_val_mask = sorted((retina_val_images + lung_val_images + lesion_val_images)), sorted((retina_val_masks + lung_val_masks + lesion_val_masks))\nfull_test_image, full_test_mask = sorted((retina_test_images + lung_test_images + lesion_test_images)), sorted((retina_test_masks + lung_test_masks + lesion_test_masks))\n\nprint(f\"\\nDataset Summary (Before Augmentation, and Transformations).\\n\")\nprint(f\"{len(full_train_image)} training images, and {len(full_train_mask)} training masks\")\nprint(f\"{len(full_val_image)} validation images, and {len(full_val_mask)} validation masks\")\nprint(f\"{len(full_test_image)} test images, and {len(full_test_mask)} test masks\\n\")\n\nprint(\"=\" * 74)\nprint(f\"||--------||  Training\\t||  Validation\\t||\\tTest\\t||     Total\\t||\")\nprint(\"-\" * 74)\nprint(f\"|| Retina ||\\t{len(retina_train_images)}\\t||\\t{len(retina_val_images)}\\t||\\t{len(retina_test_images)}\\t||\\t{R_FULL}\\t||\")\nprint(f\"|| Lungs  ||\\t{len(lung_train_images)}\\t||\\t{len(lung_val_images)}\\t||\\t{len(lung_val_images)}\\t||\\t{LU_FULL}\\t||\")\nprint(f\"|| Lesion ||\\t{len(lesion_train_images)}\\t||\\t{len(lesion_val_images)}\\t||\\t{len(lesion_test_images)}\\t||\\t{LE_FULL}\\t||\")\nprint(\"=\" * 74)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Augmentation Pipeline","metadata":{}},{"cell_type":"markdown","source":"### Important Augmentation, and visualization Functions.","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, images_paths, labels_paths, train=True, image_size=(512, 512)):\n        self.images_paths = images_paths\n        self.labels_paths = labels_paths\n        self.image_size = image_size\n        self.train = train\n\n        # Image transformation for all images; train or not train\n        self.image_transform = transforms.Compose([\n            transforms.Resize(image_size), # Resizing images first for consistency\n            transforms.ToTensor(), # Converting from '0 - 225', TO '0 - 1'\n            transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])]) # Because of backbone.\n\n        # Mask transformation for all masks; train or not train\n        self.mask_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor()]) # Converting from '0 - 225', TO '0 - 1'\n\n        self.augment_transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p = 0.5),\n            transforms.RandomVerticalFlip(p = 0.5),\n            transforms.RandomRotation(degrees = 30),\n            transforms.GaussianBlur(kernel_size = 5, sigma = (0.1, 2.0))])\n\n    def __len__(self):\n        return len(self.images_paths) * (2 if self.train else 1)\n    \n    def __getitem__(self, idx):\n        original_idx = idx // 2 if self.train else idx\n        is_augmented = self.train and idx % 2 == 1\n\n        image_path = self.images_paths[original_idx]\n        image = Image.open(image_path).convert('RGB')\n        \n        label_path = self.labels_paths[original_idx]\n        mask = Image.open(label_path)\n        # Handling situations where mask is True/False instead of 1's and 0's as observed in `CHASE_DB1/Masks`\n        mask = np.where(np.array(mask) == False, 0, 1).astype(np.uint8)\n        mask = Image.fromarray(mask)\n        \n        if is_augmented:\n            # Apply augmentation \n            seed = torch.randint(0, 2**32, (1,)).item()\n            torch.manual_seed(seed)\n            image = self.augment_transform(image)\n            torch.manual_seed(seed)\n            mask = self.augment_transform(mask)\n        \n        # Apply transformations\n        image = self.image_transform(image)\n        mask = self.mask_transform(mask)\n        \n        return image, mask.long()\n    \n    \ndef image_denormalize(tensor):\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n    tensor = tensor * std + mean\n    img_array = tensor.permute(1, 2, 0).numpy()\n    return img_array\n\ndef calculate_iou_dice(outputs, targets):\n    outputs = torch.argmax(outputs, dim=1)\n    intersection = (outputs & targets).float().sum((1, 2))\n    union = (outputs | targets).float().sum((1, 2))\n    \n    iou = (intersection + 1e-6) / (union + 1e-6)\n    dice = (2 * intersection + 1e-6) / (outputs.float().sum((1, 2)) + targets.float().sum((1, 2)) + 1e-6)\n    \n    return iou.mean(), dice.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading train data\ntrain_dataset = CustomDataset(full_train_image, full_train_mask, train = True, image_size = IMAGE_SIZE)\ntrain_loader = DataLoader(train_dataset, batch_size = 3, shuffle = True, drop_last = True, pin_memory = True)\n\n# Loading validation data\nvalidation_dataset = CustomDataset(full_val_image, full_val_mask, train = False, image_size = IMAGE_SIZE)\nvalidation_loader = DataLoader(validation_dataset, batch_size = 3, shuffle = True, drop_last = True, pin_memory = True)\n\n# Loading Test data\ntest_dataset = CustomDataset(full_test_image, full_test_mask, train = False, image_size = IMAGE_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size = 3, shuffle = True, drop_last = True, pin_memory = True)\n\nprint(f\"Dataset Summary (Post-Augmentation, and Transformations).\\n\")\nprint(\"=\" * 66)\nprint(f\"||----------------||   Training\\t||  Validation\\t||\\tTest\\t||\")\nprint(\"-\" * 66)\nprint(f\"|| Full Dataset   ||\\t{len(train_dataset)}\\t||\\t{len(validation_dataset)}\\t||\\t{len(test_dataset)}\\t||\")\nprint(f\"|| Batch Size\\t  ||\\t{len(train_dataset)//len(train_loader)}\\t||\\t{len(validation_dataset)//len(validation_loader)}\\t||\\t{len(test_dataset)//len(test_loader)}\\t||\")\nprint(f\"|| No. of Batches ||\\t{len(train_loader)}\\t||\\t{len(validation_loader)}\\t||\\t{len(test_loader)}\\t||\")\nprint(\"=\" * 66)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture Definition, Training & evaluationa","metadata":{}},{"cell_type":"markdown","source":"## Architecture Definition","metadata":{}},{"cell_type":"code","source":"class DeepLabsV3(nn.Module):\n    def __init__(self, num_classes):\n        super(DeepLabsV3, self).__init__()\n        self.model = models.segmentation.deeplabv3_resnet50(pretrained = True)\n        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size = (1, 1), stride = (1, 1))\n        \n    def forward(self, x):\n        return self.model(x)[\"out\"]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\ntorch.cuda.empty_cache()\nmodel = DeepLabsV3(NUM_CLASSES).to(device)\n# print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training, and Evaluation","metadata":{}},{"cell_type":"code","source":"# BEFORE RUNNING THIS CELL, COMMENT OUT/IN THE NEXT FEW LINES IF YOU WANT TO KEEP PREVIOUSLY SAVED BEST MODEL.\n\n####################################\ncheckpoint_path = \"best_robust.pth\"#\ntry:                               #\n    os.remove(checkpoint_path)     #\nexcept Exception:                  #\n    pass                           #\n####################################\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nbest_val_iou = float(\"inf\")\nnum_epochs = 3 # Adjust, and experiment.\naccumulation_steps = 8 # Handling OutOfMemoryError with gradient accumulation ** <-- Those are stars lol.\n\n##########################\n# Collecting to visualize\nepochs = []\ntrain_ious = []\ntrain_dices = []\ntrain_losses = []\nval_ious = []\nval_dices = []\nval_losses = []\n##########################\n\nprint(\"Training Started... LET's GOOOOOOO !!!!!\")\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    running_iou = 0.0\n    running_dice = 0.0\n    # Saving for plot.\n    epochs.append(epoch+1)\n    # Handling OutOfMemoryError\n    accumulated_steps = 0.0\n\n    for i, (images, masks) in enumerate(train_loader):\n        images = images.to(device)\n        masks = masks.to(device).squeeze(1) # Handling (3D tensors) error, \"only batches of spatial targets supported but got targets of size: :[6, 1, 512, 512]\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n\n        # Handling OutOfMemoryError\n        accumulated_steps += 1\n        # optimizer.step()\n        if accumulated_steps % accumulation_steps == 0 or i == len(train_loader) - 1:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        iou, dice = calculate_iou_dice(outputs, masks)\n\n        running_loss += loss.item() * images.size(0)\n        running_iou += iou.item() * images.size(0)\n        running_dice += dice.item() * images.size(0)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    epoch_iou = running_iou / len(train_loader.dataset)\n    epoch_dice = running_dice / len(train_loader.dataset)\n    # Saving for plot.\n    train_losses.append(epoch_loss)\n    train_ious.append(epoch_iou)\n    train_dices.append(epoch_dice)\n    model.eval()\n    val_loss = 0.0\n    val_iou = 0.0\n    val_dice = 0.0\n\n    with torch.no_grad():\n        for images, masks in validation_loader:\n            images = images.to(device)\n            masks = masks.to(device).squeeze(1) # Handling (3D tensors) error, \"only batches of spatial targets supported but got targets of size: :[6, 1, 512, 512]\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            iou, dice = calculate_iou_dice(outputs, masks)\n\n            val_loss += loss.item() * images.size(0)\n            val_iou += iou.item() * images.size(0)\n            val_dice += dice.item() * images.size(0)\n\n    val_loss = val_loss / len(val_loader.dataset)\n    val_iou = val_iou / len(val_loader.dataset)\n    val_dice = val_dice / len(val_loader.dataset)\n    # Saving for plot.\n    val_losses.append(val_loss)\n    val_ious.append(val_iou)\n    val_dices.append(val_dice)\n\n    if val_iou < val_iou:\n        best_val_iou = val_iou\n        torch.save(model.state_dict(), checkpoint_path)\n        print(\"New Best Model Saved!\")\n\n    print(f\"Epoch {epoch+1}/{num_epochs} ====== \"\n          f\"Training : (Loss: {epoch_loss:.4f} - IoU: {epoch_iou:.4f} - Dice: {epoch_dice:.4f}) *=* \"\n          f\"Validation : (Loss: {val_loss:.4f} - IoU: {val_iou:.4f} - Dice: {val_dice:.4f})\")\n    # Handling OutOfMemoryError\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.predict()\ntrain_ious","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Training Metrics","metadata":{}},{"cell_type":"code","source":"plt.subplots(1, 3, figsize = (10, 10))\n\nplt.subplot(1, 3, 1)\nplt.title(\"Train/Validation IoUs\")\nplt.plot(train_ious, label = \"Train IoU\")\nplt.plot(val_ious, label = \"Val IoU\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"IoU\")\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.title(\"Train/Validation Dice Scores\")\nplt.plot(train_dices, label = \"Train Dice\")\nplt.plot(val_dices, label = \"Val Dice\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Dice\")\nplt.legend()\n\nplt.subplot(1, 3, 3)\nplt.title(\"Train/Validation Losses\")\nplt.plot(train_losses, label = \"Train Loss\")\nplt.plot(val_losses, label = \"Val Dice\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Dice\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it = iter(train_dataset)\n# image, mask = next(it)\n\n# image = image_denormalize(image)\n# plt.imshow(mask[0], cmap = \"gray\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}